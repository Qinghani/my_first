{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fcfa71",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6704e1",
   "metadata": {},
   "source": [
    "# 6. 自然语言处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9d3cf",
   "metadata": {},
   "source": [
    "在这个教程中，我们将离开像静态图像这样的独立数据，转向依赖于序列中其它数据项的数据。作为示例，我们将使用句子文本。语言自然由序列数据组成，形式为单词中的字符和句子中的单词。其它序列数据的例子包括股票价格和天气数据等时间序列。视频虽然包含静态图像，但也属于序列。当数据中的元素与之前和之后的内容有关系，就需要采取不同的方法来处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c166f5",
   "metadata": {},
   "source": [
    "## 6.1 目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2afe7",
   "metadata": {},
   "source": [
    "* 使用分词器为神经网络准备文本\n",
    "* 观察如何使用嵌入来识别文本数据的数值特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896302a8",
   "metadata": {},
   "source": [
    "## 6.2 BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed512137",
   "metadata": {},
   "source": [
    "BERT，全称为 **B**idirectional **E**ncoder **R**epresentations from **T**ransformers，是 [Google](https://www.google.com/) 在 2018 年引入的一种开创性模型。\n",
    "\n",
    "BERT 同时针对两个目标进行训练：\n",
    "* 从一系列单词中预测缺失的单词\n",
    "* 在一系列句子之后预测新的句子\n",
    "\n",
    "让我们用这两种挑战来看看 BERT 的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4571fbb7",
   "metadata": {},
   "source": [
    "## 6.3 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76984d1a",
   "metadata": {},
   "source": [
    "由于神经网络是数值计算机，让我们将文本转换为数值 token。让我们加载 BERT 的[分词器](https://huggingface.co/docs/transformers/main_classes/tokenizer#tokenizer)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForQuestionAnswering\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221eb85",
   "metadata": {},
   "source": [
    "BERT 的`分词器`可以一次性[编码](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode)多段文本。稍后我们将测试 BERT 的记忆力，先给它一些信息和一个关于信息的问题。您可以随时回到这里，尝试不同的句子组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"I understand equations, both the simple and quadratical.\"\n",
    "text_2 = \"What kind of equations do I understand?\"\n",
    "\n",
    "# Tokenized input with special tokens around it (for BERT: [CLS] at the beginning and [SEP] at the end)\n",
    "indexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n",
    "indexed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f931f",
   "metadata": {},
   "source": [
    "如果计算 token 的数量，会发现句子中的 token 数量多于单词数。让我们来看看为什么会这样。可以用 [convert_ids_to_tokens](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_ids_to_tokens) 来查看使用了哪些 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([str(token) for token in indexed_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb501dc",
   "metadata": {},
   "source": [
    "索引列表比原始输入长的原因有两个：\n",
    "1. `tokenizer` 添加了 `special_tokens` 来表示序列的开始（`[CLS]`）和句子之间的分隔（`[SEP]`）。\n",
    "2. `tokenizer` 可以将一个词分解成多个部分。\n",
    "\n",
    "从语言学角度来看，第二点很有趣。许多语言都有[词根](https://zh.wikipedia.org/wiki/%E8%AF%8D%E6%A0%B9)，或构成单词的组成部分。例如，“quadratic”这个词的词根是“quadr”，意思是“4”。BERT 并不是用语言定义词根，而是使用 [WordPiece](https://paperswithcode.com/method/wordpiece) 模型来找出如何拆分单词模式。我们今天使用的 BERT 模型有一个 `28996` 个 token 的词汇表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc9975",
   "metadata": {},
   "source": [
    "我们可以直接[解码](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)编码过的文本。注意 `special_tokens` 已经被添加进去了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020bac7",
   "metadata": {},
   "source": [
    "## 6.4 文本分段"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48df212",
   "metadata": {},
   "source": [
    "为了使用 BERT 模型进行预测，它还需要一个 `segment_ids` 的列表。这是一个与 token 相同长度的向量，表示每个句子属于哪个段落。\n",
    "\n",
    "由于我们的 `tokenizer` 添加了一些 `special_tokens`，可以使用这些特殊标记来找到段落。首先，定义哪个索引对应哪个特殊标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = 101\n",
    "sep_token = 102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0cc070",
   "metadata": {},
   "source": [
    "接下来创建一个 `for` 循环。将从 `segment_id` 设置为 `0` 开始，并且每当看到 [SEP] 标记时就增加 `segment_id`。为了确保效果，稍后将把这些 `segment_ids` 和 `indexed_tokens` 作为张量输入模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feace8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_ids(indexed_tokens):\n",
    "    segment_ids = []\n",
    "    segment_id = 0\n",
    "    for token in indexed_tokens:\n",
    "        if token == sep_token:\n",
    "            segment_id += 1\n",
    "        segment_ids.append(segment_id)\n",
    "    segment_ids[-1] -= 1  # Last [SEP] is ignored\n",
    "    return torch.tensor([segment_ids]), torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebae44e",
   "metadata": {},
   "source": [
    "测试一下。每个数字是否正确地对应第一句和第二句？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e062f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_tensors, tokens_tensor = get_segment_ids(indexed_tokens)\n",
    "segments_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1233f43",
   "metadata": {},
   "source": [
    "## 6.4 文本掩码（Text Masking）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d81d87",
   "metadata": {},
   "source": [
    "先看看 BERT 对单词的处理。为了训练词嵌入，BERT 在一系列单词中掩掉一个单词。掩码用了一个特殊的标记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dfb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9792811",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b50ad",
   "metadata": {},
   "source": [
    "让我们从之前的两个句子中选择位置索引 `5` 进行掩码。随时回到这里改变索引，看看结果会如何变化！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8932dbda",
   "metadata": {},
   "source": [
    "接下来，应用掩码并验证它是否出现在句子序列中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deac9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens[masked_index] = tokenizer.mask_token_id\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokenizer.decode(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6aea80",
   "metadata": {},
   "source": [
    "然后，加载用于预测缺失单词的模型：`modelForMaskedLM`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908dd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_model = BertForMaskedLM.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b606a7b",
   "metadata": {},
   "source": [
    "就像使用其它 PyTorch 模块一样，我们可以检查其架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78313cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00505af9",
   "metadata": {},
   "source": [
    "您能找到标有 `word_embeddings` 的部分吗？这些是 BERT 为每个 token 学习到的嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ff0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_table = next(masked_lm_model.bert.embeddings.word_embeddings.parameters())\n",
    "embedding_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c8ad3",
   "metadata": {},
   "source": [
    "我们可以验证 BERT 词汇表中的 `28996` 个 token 都有一个大小为 `768` 的嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ba827",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020544fd",
   "metadata": {},
   "source": [
    "测试一下模型吧！它能正确预测我们提供的句子中缺失的单词吗？我们将使用 [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html) 来让 PyTorch 不计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef11d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = masked_lm_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce11d45",
   "metadata": {},
   "source": [
    "这有点难读懂，来看一下 `shape`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4efa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc56db7",
   "metadata": {},
   "source": [
    "`23` 指的是是 token 数，`28996` 是指对 BERT 词汇表中每个 token 的预测。我们想找到词汇表中所有 token 的最大值，可以用 [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803815c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted token\n",
    "predicted_index = torch.argmax(predictions[0][0], dim=1)[masked_index].item()\n",
    "predicted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8f7da",
   "metadata": {},
   "source": [
    "让我们看看 token `1241` 对应的是什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc45267",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "predicted_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066afef9",
   "metadata": {},
   "source": [
    "您觉得怎么样？正确吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56a36e",
   "metadata": {},
   "source": [
    "## 6.5 问题与回答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83910deb",
   "metadata": {},
   "source": [
    "虽然词掩码很有趣，但 BERT 是为更复杂的问题设计的，比如句子预测。它能通过 [Attention Transformer](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) 架构来完成这一任务。\n",
    "\n",
    "这部分将使用 BERT 的不同版本，它有自己的 tokenizer。让我们为示例句子找到一组新的 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"I understand equations, both the simple and quadratical.\"\n",
    "text_2 = \"What kind of equations do I understand?\"\n",
    "\n",
    "question_answering_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "indexed_tokens = question_answering_tokenizer.encode(text_1, text_2, add_special_tokens=True)\n",
    "segments_tensors, tokens_tensor = get_segment_ids(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a87b3",
   "metadata": {},
   "source": [
    "接下来加载 `question_answering_model`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answering_model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1695c7d",
   "metadata": {},
   "source": [
    "可以像在掩掉单词时一样输入 tokens 和 segments。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ce72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the start and end positions logits\n",
    "with torch.no_grad():\n",
    "    out = question_answering_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e19220",
   "metadata": {},
   "source": [
    "`question_answering_model` 和问答模型正在扫描我们的输入序列，以找到最能回答问题的子序列。数值越高，答案就越有可能是从这里开始的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e778f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.start_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8e771",
   "metadata": {},
   "source": [
    "同样，`end_logits` 中的数越高，答案就越可能结束在那个 token 上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1db4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e21de1",
   "metadata": {},
   "source": [
    "然后可以用 [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html) 来找到从开始到结束的 `answer_sequence`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d391ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_sequence = indexed_tokens[torch.argmax(out.start_logits):torch.argmax(out.end_logits)+1]\n",
    "answer_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b0ae9",
   "metadata": {},
   "source": [
    "最后，[解码](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)这些 token，看看答案是否正确！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answering_tokenizer.convert_ids_to_tokens(answer_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7287ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answering_tokenizer.decode(answer_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbc259",
   "metadata": {},
   "source": [
    "## 6.7 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1545df",
   "metadata": {},
   "source": [
    "干得好！您成功地使用了大语言模型 (LLM) 从一系列句子中提取答案。尽管 BERT 在首次发布时是最先进的，但许多其它 LLM 自那以后也取得了突破。[build.nvidia.com](https://build.nvidia.com/explore/discover) 上托管了许多这样的模型，可以在浏览器中与它们互动。去探索一下，了解了解当今的最先进技术吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08d9e5",
   "metadata": {},
   "source": [
    "### 6.7.1 清理内存\n",
    "继续进行后续内容之前，请执行以下单元清理 GPU 显存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9752608",
   "metadata": {},
   "source": [
    "### 6.7.2 下一步"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2d0ce",
   "metadata": {},
   "source": [
    "恭喜您，您已经完成了所有课程学习目标！\n",
    "\n",
    "作为最后的练习，请在评估中成功完成一个端到端的图像分类问题，完成后您将获得本课程的证书。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
